cue.proxy = tcp -h cuetest01-vm -p 9019 -t 10000:tcp -h cuetest02-vm -p 9019 -t 10000:tcp -h cuetest03-vm -p 9019 -t 10000
spring.velocity.checkTemplateLocation=false

datasource.cue-data-source.driver-class-name=org.postgresql.Driver
datasource.cue-data-source.jdbc-url=jdbc:postgresql://dbhost/dbname
datasource.cue-data-source.username=cue
datasource.cue-data-source.password=password

# HikariCP Configuration
# Maximum lifetime of a connection in milliseconds (5 hours = 18000000 ms)
# This should be shorter than PostgreSQL's connection timeout
datasource.cue-data-source.max-lifetime=18000000

# Maximum time to wait for a connection from the pool (30 seconds)
datasource.cue-data-source.connection-timeout=30000

# Maximum time a connection can remain idle (10 minutes)
datasource.cue-data-source.idle-timeout=600000

# Minimum number of idle connections to maintain
datasource.cue-data-source.minimum-idle=5

# Maximum number of connections in the pool
datasource.cue-data-source.maximum-pool-size=20

# Connection test query to validate connections
datasource.cue-data-source.connection-test-query=SELECT 1

# How often to check for idle connections that can be evicted (30 seconds)
datasource.cue-data-source.leak-detection-threshold=30000

grpc.cue_port=${CUEBOT_GRPC_CUE_PORT:8443}
grpc.rqd_server_port=${CUEBOT_GRPC_RQD_SERVER_PORT:8444}
grpc.max_message_bytes=104857600
# Number of entries allowed in the RQD channel cache
grpc.rqd_cache_size=2000
# RQD Channel Cache Expiration in Minutes
grpc.rqd_cache_expiration=5
# RQD Channel Cache expected concurrency
grpc.rqd_cache_concurrency=20
# RQD Channel task deadline in seconds
grpc.rqd_task_deadline=10

# Healthy Threadpool Executor
booking_queue.threadpool.health_threshold=10
booking_queue.threadpool.core_pool_size=10
booking_queue.threadpool.max_pool_size=14
booking_queue.threadpool.queue_capacity=2000
dispatch.threadpool.core_pool_size=6
dispatch.threadpool.max_pool_size=8
dispatch.threadpool.queue_capacity=2000
healthy_threadpool.health_threshold=6
healthy_threadpool.min_unhealthy_period_min=3
report_queue.threadPoolSizeInitial=6
report_queue.threadPoolSizeMax=12
# The queue size should be bigger then the expected amount of hosts
report_queue.queueSize=5000
kill_queue.threadPoolSizeInitial=2
kill_queue.threadPoolSizeMax=6
kill_queue.queueSize=1000

# Turn on/off jobCompletion mailing module
mailing.enabled=true

# Whether or not to enable publishing to a messaging topic.
# Set to a boolean value. See com/imageworks/spcue/services/JmsMover.java.
messaging.enabled=false

# Default root directory for which logs will be stored if no other OS is defined.
# See com/imageworks/spcue/util/JobLogUtil.java.
# Override this via environment variable (CUE_FRAME_LOG_DIR) or command line flag
# (--log.frame-log-root.default_os). Command line flag will be preferred if both are provided.
log.frame-log-root.default_os=${CUE_FRAME_LOG_DIR:/shots}
# To set up root directories for other OS create new environment
# variable as `log.frame-log-root.[OS] where OS relates to str_os on the job table
# For example:
#   - log.frame-log-root.linux=${CUE_FRAME_LOG_DIR:/shots}
#   - log.frame-log-root.Windows=${CUE_FRAME_LOG_DIR:/S:}
# Note that for Windows, either forward or back slashes will work. However if CUE_FRAME_LOG_DIR is empty
# and S directory is in the root, the path will be broken due to the slash in front of S. Hence, if you
# are planning to use a folder in the root, use:
#   - log.frame-log-root.Windows=${S:}

# Loki
# To enable rqd frame logs to Loki configure the url as shown below. When Loki
# rqd frame logs are enable all frame logs will be streamed to the loki server instead of using the
# CUE_FRAME_LOG_DIR filesystem path. Refer to the documentation on how to configure Loki.

# This is the base url of the Loki server. If the url is not reachable, rqd will fail running frames. If it is empty
# it will not use the Loki backend for logging
log.loki.url=

# Maximum number of jobs to query.
dispatcher.job_query_max=20
# Number of seconds before waiting to book the same job from a different host.
# "0" disables the job_lock
dispatcher.job_lock_expire_seconds=20
# Concurrency level to allow on the job lock cache
dispatcher.job_lock_concurrency_level=14
# Maximum number of frames to query from the DB to attempt to dispatch.
dispatcher.frame_query_max=20
# Maximum number of frames to book at one time on the same host.
dispatcher.job_frame_dispatch_max=8
# Maximum number of frames to dispatch from a host at one time.
dispatcher.host_frame_dispatch_max=12
# Choose between different scheduling strategies:
#  - PRIORITY_ONLY: Sort by priority only
#  - FIFO: Whether or not to enable FIFO scheduling in the same priority.
#  - BALANCED: Use a rank formula that takes into account time waiting, and number
#      of cores required: rank = priority + (100 * (1 - (job.cores/job.int_min_cores))) + age in days
#      layer limiting is also disabled in this mode for performance reasons
dispatcher.scheduling_mode=PRIORITY_ONLY

# Number of threads to keep in the pool for launching job.
dispatcher.launch_queue.core_pool_size=1
# Maximum number of threads to allow in the pool for launching job.
dispatcher.launch_queue.max_pool_size=1
# Queue capacity for launching job.
dispatcher.launch_queue.queue_capacity=100

# Number of threads to keep in the pool for various operation.
dispatcher.dispatch_pool.core_pool_size=4
# Maximum number of threads to allow in the pool for various operation.
dispatcher.dispatch_pool.max_pool_size=4
# Queue capacity for various operation.
dispatcher.dispatch_pool.queue_capacity=500

# Number of threads to keep in the pool for management operation.
dispatcher.manage_pool.core_pool_size=8
# Maximum number of threads to allow in the pool for management operation.
dispatcher.manage_pool.max_pool_size=8
# Queue capacity for management operation.
dispatcher.manage_pool.queue_capacity=250

# Number of threads to keep in the pool for handling Host Report.
dispatcher.report_queue.core_pool_size=6
# Maximum number of threads to allow in the pool for handling Host Report.
dispatcher.report_queue.max_pool_size=8
# Queue capacity for handling Host Report.
dispatcher.report_queue.queue_capacity=1000

# The minimum amount of available space on the temporary directory (mcp) to book a host.
# Hosts with less free space than the limit will be marked as REPAIR
# If equals to -1, it means the feature is turned off
dispatcher.min_available_temp_storage_percentage=5

# Number of threads to keep in the pool for kill frame operation.
dispatcher.kill_queue.core_pool_size=6
# Maximum number of threads to allow in the pool for kill frame operation.
dispatcher.kill_queue.max_pool_size=8
# Queue capacity for kill frame operation.
dispatcher.kill_queue.queue_capacity=1000

# Number of threads to keep in the pool for booking.
dispatcher.booking_queue.core_pool_size=6
# Maximum number of threads to allow in the pool for booking.
dispatcher.booking_queue.max_pool_size=6
# Queue capacity for booking.
dispatcher.booking_queue.queue_capacity=1000

# Percentage of used memory to consider a risk for triggering oom-killer
# If equals to -1, it means the feature is turned off
dispatcher.oom_max_safe_used_physical_memory_threshold=0.9

# Percentage of used swap to consider a risk for triggering oom-killer
# If equals to -1, it means the feature is turned off
dispatcher.oom_max_safe_used_swap_memory_threshold=0.05

# How much can a frame exceed its reserved memory.
#  - 0.5 means 50% above reserve
#  - -1.0 makes the feature inactive
# This feature is being kept inactive for now as we work on improving the
#  frame retry logic (See commit comment for more details).
dispatcher.oom_frame_overboard_allowed_threshold=-1.0

# How many times should cuebot send a kill request for the same frame-host before reporting
# the frame as stuck
dispatcher.frame_kill_retry_limit=3

# The default amount of memory reserved for a frame if no memory
# reservation settings are specified.
# Default = 4GB
dispatcher.memory.mem_reserved_default = 3355443

# The maximum amount of memory that can be requested for a given frame.
# Default = 50GB
dispatcher.memory.mem_reserved_max = 52428800

# The minimum amount of memory that can be assigned to a frame.
# Default = 250MB
dispatcher.memory.mem_reserved_min = 262144

# Memory reserved by system, gets chopped off the available memory
# Default = 500MB
dispatcher.memory.mem_reserved_system = 524288

# The default amount of gpu memory reserved for a frame if no gpu memory
# reservation settings are specified
dispatcher.memory.mem_gpu_reserved_default = 0

# The minimum amount of gpu memory that can be assigned to a frame.
dispatcher.memory.mem_gpu_reserved_min = 0

# The maximum amount of gpu memory that can be assigned to a frame.
# Default = 100GB
dispatcher.memory.mem_gpu_reserved_max = 104857600

# Whether to satisfy dependents (*_ON_FRAME and *_ON_LAYER) only on Frame success
depend.satisfy_only_on_frame_success=true

# Jobs will be archived to the history tables after being completed for this long.
history.archive_jobs_cutoff_hours=72

# Delete down hosts automatically.
maintenance.auto_delete_down_hosts=false

# Set hostname/IP of the smtp host. Will be used for mailing
smtp_host=smtp

# Flags related to a job that runs periodically to deactivate shows that haven't been
#  receiving jobs.
# A comma separated list of shows that won't be deactivated by the scheduled tasks
protected_shows=testing
# Number of days a show needs to be stale before it gets deactivated.
#  -1 means shows should not get deactivated at all.
max_show_stale_days=-1

# These flags determine whether layers/frames will be readonly when job is finished.
# If flags are set as true, layers/frames cannot be retried, eaten, edited dependency on, etc.
layer.finished_jobs_readonly=false
frame.finished_jobs_readonly=false

# Enable Prometheus metrics collecting module
metrics.prometheus.collector=false
# What environment variable to use to acquire the deployment environment id (et. dev, prod, staging)
metrics.prometheus.environment_id.environment_variable=DEPLOYMENT_ENVIRONMENT
